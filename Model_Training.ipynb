{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stumbleupon_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xcY8QmWq5KO"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "import numpy as np\n",
        "from torchtext import data\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import Vectors, GloVe\n",
        "\n",
        "def load_dataset(test_sen=None):\n",
        "\n",
        "    \"\"\"\n",
        "    tokenizer : Breaks sentences into a list of words. If sequential=False, no tokenization is applied\n",
        "    Field : A class that stores information about the way of preprocessing\n",
        "    fix_length : An important property of TorchText is that we can let the input to be variable length, and TorchText will\n",
        "                 dynamically pad each sequence to the longest sequence in that \"batch\". But here we are using fi_length which\n",
        "                 will pad each sequence to have a fix length of 200.\n",
        "                 \n",
        "    build_vocab : It will first make a vocabulary or dictionary mapping all the unique words present in the train_data to an\n",
        "                  idx and then after it will use GloVe word embedding to map the index to the corresponding word embedding.\n",
        "                  \n",
        "    vocab.vectors : This returns a torch tensor of shape (vocab_size x embedding_dim) containing the pre-trained word embeddings.\n",
        "    BucketIterator : Defines an iterator that batches examples of similar lengths together to minimize the amount of padding needed.\n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    tokenize = lambda x: x.split()\n",
        "    TEXT = data.Field(sequential=True, tokenize=tokenize, lower=True, include_lengths=True, batch_first=True, fix_length=200)\n",
        "    LABEL = data.LabelField()\n",
        "    fields = [('boilerplate',TEXT),('label', LABEL)]\n",
        "\n",
        "\n",
        "    #loading dataset\n",
        "    training_data=data.TabularDataset(path = 'processed.csv',format = 'csv',fields = fields,skip_header = True)\n",
        "\n",
        "    train_data, test_data = training_data.split(split_ratio=0.32)\n",
        "    \n",
        "    TEXT.build_vocab(train_data, vectors=GloVe(name='6B', dim=300))\n",
        "    LABEL.build_vocab(train_data)\n",
        "\n",
        "    word_embeddings = TEXT.vocab.vectors\n",
        "    print (\"Length of Text Vocabulary: \" + str(len(TEXT.vocab)))\n",
        "    print (\"Vector size of Text Vocabulary: \", TEXT.vocab.vectors.size())\n",
        "    print (\"Label Length: \" + str(len(LABEL.vocab)))\n",
        "\n",
        "    train_data, valid_data = train_data.split() # Further splitting of training_data to create new training_data & validation_data\n",
        "    train_iter, valid_iter, test_iter = data.BucketIterator.splits((train_data, valid_data, test_data), batch_size=32, sort_key=lambda x: len(x.boilerplate), repeat=False, shuffle=True)\n",
        "\n",
        "    vocab_size = len(TEXT.vocab)\n",
        "\n",
        "    return TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnY0uVdVDLa3"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class LSTMClassifier(nn.Module):\n",
        "\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n",
        "\t\tsuper(LSTMClassifier, self).__init__()\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\tArguments\n",
        "\t\t---------\n",
        "\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n",
        "\t\toutput_size : 2 = (pos, neg)\n",
        "\t\thidden_sie : Size of the hidden_state of the LSTM\n",
        "\t\tvocab_size : Size of the vocabulary containing unique words\n",
        "\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n",
        "\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself.output_size = output_size\n",
        "\t\tself.hidden_size = hidden_size\n",
        "\t\tself.vocab_size = vocab_size\n",
        "\t\tself.embedding_length = embedding_length\n",
        "\t\t\n",
        "\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)# Initializing the look-up table.\n",
        "\t\tself.word_embeddings.weight = nn.Parameter(weights, requires_grad=False) # Assigning the look-up table to the pre-trained GloVe word embedding.\n",
        "\t\tself.lstm = nn.LSTM(embedding_length, hidden_size)\n",
        "\t\tself.label = nn.Linear(hidden_size, output_size)\n",
        "\t\t\n",
        "\tdef forward(self, input_sentence, batch_size=None):\n",
        "\t\n",
        "\t\t\"\"\" \n",
        "\t\tParameters\n",
        "\t\t----------\n",
        "\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n",
        "\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n",
        "\t\t\n",
        "\t\tReturns\n",
        "\t\t-------\n",
        "\t\tOutput of the linear layer containing logits for positive & negative class which receives its input as the final_hidden_state of the LSTM\n",
        "\t\tfinal_output.shape = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\t\"\"\"\n",
        "\t\t\n",
        "\t\t''' Here we will map all the indexes present in the input sequence to the corresponding word vector using our pre-trained word_embeddings.'''\n",
        "\t\tinput = self.word_embeddings(input_sentence) # embedded input of shape = (batch_size, num_sequences,  embedding_length)\n",
        "\t\tinput = input.permute(1, 0, 2) # input.size() = (num_sequences, batch_size, embedding_length)\n",
        "\t\tif batch_size is None:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial hidden state of the LSTM\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) # Initial cell state of the LSTM\n",
        "\t\telse:\n",
        "\t\t\th_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\t\tc_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n",
        "\t\toutput, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0))\n",
        "\t\tfinal_output = self.label(final_hidden_state[-1]) # final_hidden_state.size() = (1, batch_size, hidden_size) & final_output.size() = (batch_size, output_size)\n",
        "\t\t\n",
        "\t\treturn final_output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iW86pVCPMit",
        "outputId": "b6546b94-ab05-48c5-c5f7-f39fdcea51b6"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "# import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "# import load_data\n",
        "# from models.LSTM import LSTMClassifier\n",
        "\n",
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_dataset()\n",
        "\n",
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "    \n",
        "def train_model(model, train_iter, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.cuda()\n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        boilerplate = batch.boilerplate[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            boilerplate = boilerplate.cuda()\n",
        "            target = target.cuda()\n",
        "        if (boilerplate.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        optim.zero_grad()\n",
        "        prediction = model(boilerplate)\n",
        "        loss = loss_fn(prediction, target)\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optim.step()\n",
        "        steps += 1\n",
        "        \n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "        \n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "        \n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            boilerplate = batch.boilerplate[0]\n",
        "            if (boilerplate.size()[0] is not 32):\n",
        "                continue\n",
        "            target = batch.label\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                boilerplate = boilerplate.cuda()\n",
        "                target = target.cuda()\n",
        "            prediction = model(boilerplate)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n",
        "\t\n",
        "\n",
        "# learning_rate = 2e-5\n",
        "learning_rate = .01\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "loss_fn = F.cross_entropy\n",
        "# loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "for epoch in range(30):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc = eval_model(model, valid_iter)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        "    \n",
        "test_loss, test_acc = eval_model(model, test_iter)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Text Vocabulary: 29846\n",
            "Vector size of Text Vocabulary:  torch.Size([29846, 300])\n",
            "Label Length: 2\n",
            "Epoch: 01, Train Loss: 0.603, Train Acc: 63.16%, Val. Loss: 0.564506, Val. Acc: 72.17%\n",
            "Epoch: 02, Train Loss: 0.507, Train Acc: 76.20%, Val. Loss: 0.534926, Val. Acc: 72.17%\n",
            "Epoch: 03, Train Loss: 0.509, Train Acc: 74.87%, Val. Loss: 0.514947, Val. Acc: 72.02%\n",
            "Epoch: 04, Train Loss: 0.492, Train Acc: 75.53%, Val. Loss: 0.524570, Val. Acc: 72.32%\n",
            "Epoch: 05, Train Loss: 0.457, Train Acc: 78.39%, Val. Loss: 0.511250, Val. Acc: 72.47%\n",
            "Epoch: 06, Train Loss: 0.424, Train Acc: 80.25%, Val. Loss: 0.529020, Val. Acc: 72.47%\n",
            "Epoch: 07, Train Loss: 0.380, Train Acc: 82.31%, Val. Loss: 0.549017, Val. Acc: 72.02%\n",
            "Epoch: 08, Train Loss: 0.350, Train Acc: 83.31%, Val. Loss: 0.592984, Val. Acc: 70.09%\n",
            "Epoch: 09, Train Loss: 0.360, Train Acc: 82.78%, Val. Loss: 0.633140, Val. Acc: 65.62%\n",
            "Epoch: 10, Train Loss: 0.316, Train Acc: 84.71%, Val. Loss: 0.602505, Val. Acc: 73.21%\n",
            "Epoch: 11, Train Loss: 0.299, Train Acc: 86.04%, Val. Loss: 0.727125, Val. Acc: 72.17%\n",
            "Epoch: 12, Train Loss: 0.278, Train Acc: 86.50%, Val. Loss: 0.680842, Val. Acc: 71.73%\n",
            "Epoch: 13, Train Loss: 0.271, Train Acc: 86.30%, Val. Loss: 0.703179, Val. Acc: 71.28%\n",
            "Epoch: 14, Train Loss: 0.255, Train Acc: 87.57%, Val. Loss: 0.710064, Val. Acc: 69.79%\n",
            "Epoch: 15, Train Loss: 0.258, Train Acc: 87.17%, Val. Loss: 0.720953, Val. Acc: 73.96%\n",
            "Epoch: 16, Train Loss: 0.260, Train Acc: 87.70%, Val. Loss: 0.777320, Val. Acc: 70.09%\n",
            "Epoch: 17, Train Loss: 0.259, Train Acc: 87.97%, Val. Loss: 0.753442, Val. Acc: 72.77%\n",
            "Epoch: 18, Train Loss: 0.242, Train Acc: 88.10%, Val. Loss: 0.802311, Val. Acc: 71.58%\n",
            "Epoch: 19, Train Loss: 0.252, Train Acc: 88.43%, Val. Loss: 0.717393, Val. Acc: 72.62%\n",
            "Epoch: 20, Train Loss: 0.240, Train Acc: 87.77%, Val. Loss: 0.801042, Val. Acc: 73.36%\n",
            "Epoch: 21, Train Loss: 0.241, Train Acc: 88.90%, Val. Loss: 0.743656, Val. Acc: 73.96%\n",
            "Epoch: 22, Train Loss: 0.237, Train Acc: 88.70%, Val. Loss: 0.705760, Val. Acc: 73.96%\n",
            "Epoch: 23, Train Loss: 0.243, Train Acc: 88.90%, Val. Loss: 0.776383, Val. Acc: 72.77%\n",
            "Epoch: 24, Train Loss: 0.229, Train Acc: 89.49%, Val. Loss: 0.831277, Val. Acc: 71.58%\n",
            "Epoch: 25, Train Loss: 0.228, Train Acc: 89.23%, Val. Loss: 0.853484, Val. Acc: 72.17%\n",
            "Epoch: 26, Train Loss: 0.225, Train Acc: 89.10%, Val. Loss: 0.888751, Val. Acc: 70.39%\n",
            "Epoch: 27, Train Loss: 0.232, Train Acc: 88.56%, Val. Loss: 0.852001, Val. Acc: 66.52%\n",
            "Epoch: 28, Train Loss: 0.226, Train Acc: 89.76%, Val. Loss: 0.818669, Val. Acc: 73.96%\n",
            "Epoch: 29, Train Loss: 0.221, Train Acc: 89.76%, Val. Loss: 0.767467, Val. Acc: 73.07%\n",
            "Epoch: 30, Train Loss: 0.218, Train Acc: 89.63%, Val. Loss: 0.858225, Val. Acc: 71.28%\n",
            "Test Loss: 0.895, Test Acc: 73.12%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        },
        "id": "RGkAcDyOR4sr",
        "outputId": "efe44be9-3d99-49ad-bbe6-8b39bef69324"
      },
      "source": [
        "prediction = model(boilerplate)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-d6e7bd0185b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboilerplate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'boilerplate' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JiUfPyIVKIXG"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA1MnUZJPauH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c6b7e3-359d-44bc-d8e1-48869ff6fc78"
      },
      "source": [
        "import os\n",
        "import time\n",
        "import torch\n",
        "# import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "# import load_data\n",
        "# from models.LSTM import LSTMClassifier\n",
        "\n",
        "TEXT, vocab_size, word_embeddings, train_iter, valid_iter, test_iter = load_dataset()\n",
        "\n",
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "    \n",
        "def train_model(model, train_iter, epoch):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.cuda()\n",
        "    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
        "    steps = 0\n",
        "    model.train()\n",
        "    for idx, batch in enumerate(train_iter):\n",
        "        boilerplate = batch.boilerplate[0]\n",
        "        target = batch.label\n",
        "        target = torch.autograd.Variable(target).long()\n",
        "        if torch.cuda.is_available():\n",
        "            boilerplate = boilerplate.cuda()\n",
        "            target = target.cuda()\n",
        "        if (boilerplate.size()[0] is not 32):# One of the batch returned by BucketIterator has length different than 32.\n",
        "            continue\n",
        "        optim.zero_grad()\n",
        "        prediction = model(boilerplate)\n",
        "        loss = loss_fn(prediction, target)\n",
        "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "        acc = 100.0 * num_corrects/len(batch)\n",
        "        loss.backward()\n",
        "        clip_gradient(model, 1e-1)\n",
        "        optim.step()\n",
        "        steps += 1\n",
        "        \n",
        "        if steps % 100 == 0:\n",
        "            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n",
        "        \n",
        "        total_epoch_loss += loss.item()\n",
        "        total_epoch_acc += acc.item()\n",
        "        \n",
        "    return total_epoch_loss/len(train_iter), total_epoch_acc/len(train_iter)\n",
        "\n",
        "def eval_model(model, val_iter):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "    model.eval()\n",
        "\n",
        "    \n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(val_iter):\n",
        "            boilerplate = batch.boilerplate[0]\n",
        "            if (boilerplate.size()[0] is not 32):\n",
        "                continue\n",
        "            target = batch.label\n",
        "            target = torch.autograd.Variable(target).long()\n",
        "            if torch.cuda.is_available():\n",
        "                boilerplate = boilerplate.cuda()\n",
        "                target = target.cuda()\n",
        "            \n",
        "            prediction = model(boilerplate)\n",
        "            loss = loss_fn(prediction, target)\n",
        "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "\n",
        "            y_true.extend(target.tolist())\n",
        "            a=torch.max(prediction, 1)[1].view(target.size()).data\n",
        "            y_pred.extend(a.tolist())\n",
        "\n",
        "    print('Classification Report:')\n",
        "    print(classification_report(y_true, y_pred, labels=[0,1], digits=4))\n",
        "   \n",
        "    print(len(y_pred))\n",
        "    print(len(y_true))\n",
        "    return total_epoch_loss/len(val_iter), total_epoch_acc/len(val_iter)\n",
        "\t\n",
        "\n",
        "# learning_rate = 2e-5\n",
        "learning_rate = .01\n",
        "batch_size = 32\n",
        "output_size = 2\n",
        "hidden_size = 256\n",
        "embedding_length = 300\n",
        "\n",
        "model = LSTMClassifier(batch_size, output_size, hidden_size, vocab_size, embedding_length, word_embeddings)\n",
        "\n",
        "class F1_Loss(nn.Module):\n",
        "    def __init__(self, epsilon=1e-7):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        \n",
        "    def forward(self, y_pred, y_true,):\n",
        "        assert y_pred.ndim == 2\n",
        "        assert y_true.ndim == 1\n",
        "        y_true = F.one_hot(y_true, 2).to(torch.float32)\n",
        "        y_pred = F.softmax(y_pred, dim=1)\n",
        "        \n",
        "        tp = (y_true * y_pred).sum(dim=0).to(torch.float32)\n",
        "        tn = ((1 - y_true) * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "        fp = ((1 - y_true) * y_pred).sum(dim=0).to(torch.float32)\n",
        "        fn = (y_true * (1 - y_pred)).sum(dim=0).to(torch.float32)\n",
        "\n",
        "        precision = tp / (tp + fp + self.epsilon)\n",
        "        recall = tp / (tp + fn + self.epsilon)\n",
        "        \n",
        "        f1 = 2* (precision*recall) / (precision + recall + self.epsilon)\n",
        "        f1 = f1.clamp(min=self.epsilon, max=1-self.epsilon)\n",
        "        return 1 - f1.mean()\n",
        "\n",
        "\n",
        "loss_fn = F1_Loss().cuda()\n",
        "\n",
        "for epoch in range(25):\n",
        "    train_loss, train_acc = train_model(model, train_iter, epoch)\n",
        "    val_loss, val_acc = eval_model(model, valid_iter)\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')\n",
        "    \n",
        "test_loss, test_acc = eval_model(model, test_iter)\n",
        "print(f'Test Loss: {test_loss:.3f}, Test Acc: {test_acc:.2f}%')\n",
        "\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of Text Vocabulary: 29846\n",
            "Vector size of Text Vocabulary:  torch.Size([29846, 300])\n",
            "Label Length: 2\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8312    0.7399    0.7829       346\n",
            "           1     0.7289    0.8231    0.7732       294\n",
            "\n",
            "    accuracy                         0.7781       640\n",
            "   macro avg     0.7800    0.7815    0.7780       640\n",
            "weighted avg     0.7842    0.7781    0.7784       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 01, Train Loss: 0.332, Train Acc: 67.55%, Val. Loss: 0.230925, Val. Acc: 74.11%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7865    0.7775    0.7820       346\n",
            "           1     0.7416    0.7517    0.7466       294\n",
            "\n",
            "    accuracy                         0.7656       640\n",
            "   macro avg     0.7641    0.7646    0.7643       640\n",
            "weighted avg     0.7659    0.7656    0.7657       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 02, Train Loss: 0.201, Train Acc: 78.39%, Val. Loss: 0.240988, Val. Acc: 72.92%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8190    0.7457    0.7806       346\n",
            "           1     0.7292    0.8061    0.7658       294\n",
            "\n",
            "    accuracy                         0.7734       640\n",
            "   macro avg     0.7741    0.7759    0.7732       640\n",
            "weighted avg     0.7778    0.7734    0.7738       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 03, Train Loss: 0.195, Train Acc: 78.72%, Val. Loss: 0.232887, Val. Acc: 73.66%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8636    0.6040    0.7109       346\n",
            "           1     0.6558    0.8878    0.7543       294\n",
            "\n",
            "    accuracy                         0.7344       640\n",
            "   macro avg     0.7597    0.7459    0.7326       640\n",
            "weighted avg     0.7682    0.7344    0.7308       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 04, Train Loss: 0.235, Train Acc: 75.33%, Val. Loss: 0.276382, Val. Acc: 69.94%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7959    0.7890    0.7925       346\n",
            "           1     0.7542    0.7619    0.7580       294\n",
            "\n",
            "    accuracy                         0.7766       640\n",
            "   macro avg     0.7751    0.7755    0.7752       640\n",
            "weighted avg     0.7768    0.7766    0.7766       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 05, Train Loss: 0.204, Train Acc: 78.06%, Val. Loss: 0.231438, Val. Acc: 73.96%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7878    0.7832    0.7855       346\n",
            "           1     0.7466    0.7517    0.7492       294\n",
            "\n",
            "    accuracy                         0.7688       640\n",
            "   macro avg     0.7672    0.7675    0.7673       640\n",
            "weighted avg     0.7689    0.7688    0.7688       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 06, Train Loss: 0.198, Train Acc: 78.39%, Val. Loss: 0.238247, Val. Acc: 73.21%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8024    0.7630    0.7822       346\n",
            "           1     0.7363    0.7789    0.7570       294\n",
            "\n",
            "    accuracy                         0.7703       640\n",
            "   macro avg     0.7694    0.7710    0.7696       640\n",
            "weighted avg     0.7721    0.7703    0.7706       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 07, Train Loss: 0.191, Train Acc: 79.19%, Val. Loss: 0.240979, Val. Acc: 73.36%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8361    0.7370    0.7834       346\n",
            "           1     0.7284    0.8299    0.7758       294\n",
            "\n",
            "    accuracy                         0.7797       640\n",
            "   macro avg     0.7822    0.7835    0.7796       640\n",
            "weighted avg     0.7866    0.7797    0.7799       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 08, Train Loss: 0.195, Train Acc: 78.92%, Val. Loss: 0.233151, Val. Acc: 74.26%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7654    0.7919    0.7784       346\n",
            "           1     0.7447    0.7143    0.7292       294\n",
            "\n",
            "    accuracy                         0.7562       640\n",
            "   macro avg     0.7550    0.7531    0.7538       640\n",
            "weighted avg     0.7559    0.7562    0.7558       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 09, Train Loss: 0.197, Train Acc: 78.72%, Val. Loss: 0.254126, Val. Acc: 72.02%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7291    0.8555    0.7872       346\n",
            "           1     0.7863    0.6259    0.6970       294\n",
            "\n",
            "    accuracy                         0.7500       640\n",
            "   macro avg     0.7577    0.7407    0.7421       640\n",
            "weighted avg     0.7554    0.7500    0.7458       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 10, Train Loss: 0.218, Train Acc: 76.80%, Val. Loss: 0.271309, Val. Acc: 71.43%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7615    0.8121    0.7860       346\n",
            "           1     0.7601    0.7007    0.7292       294\n",
            "\n",
            "    accuracy                         0.7609       640\n",
            "   macro avg     0.7608    0.7564    0.7576       640\n",
            "weighted avg     0.7609    0.7609    0.7599       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 11, Train Loss: 0.201, Train Acc: 78.19%, Val. Loss: 0.254227, Val. Acc: 72.47%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7628    0.8179    0.7894       346\n",
            "           1     0.7658    0.7007    0.7318       294\n",
            "\n",
            "    accuracy                         0.7641       640\n",
            "   macro avg     0.7643    0.7593    0.7606       640\n",
            "weighted avg     0.7642    0.7641    0.7629       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 12, Train Loss: 0.204, Train Acc: 78.06%, Val. Loss: 0.247664, Val. Acc: 72.77%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7881    0.7630    0.7753       346\n",
            "           1     0.7311    0.7585    0.7446       294\n",
            "\n",
            "    accuracy                         0.7609       640\n",
            "   macro avg     0.7596    0.7608    0.7600       640\n",
            "weighted avg     0.7619    0.7609    0.7612       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 13, Train Loss: 0.201, Train Acc: 78.32%, Val. Loss: 0.248775, Val. Acc: 72.47%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7441    0.8150    0.7779       346\n",
            "           1     0.7548    0.6701    0.7099       294\n",
            "\n",
            "    accuracy                         0.7484       640\n",
            "   macro avg     0.7494    0.7425    0.7439       640\n",
            "weighted avg     0.7490    0.7484    0.7467       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 14, Train Loss: 0.239, Train Acc: 75.07%, Val. Loss: 0.266220, Val. Acc: 71.28%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7428    0.8179    0.7785       346\n",
            "           1     0.7568    0.6667    0.7089       294\n",
            "\n",
            "    accuracy                         0.7484       640\n",
            "   macro avg     0.7498    0.7423    0.7437       640\n",
            "weighted avg     0.7492    0.7484    0.7465       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 15, Train Loss: 0.279, Train Acc: 72.74%, Val. Loss: 0.268748, Val. Acc: 71.28%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7296    0.8266    0.7751       346\n",
            "           1     0.7581    0.6395    0.6937       294\n",
            "\n",
            "    accuracy                         0.7406       640\n",
            "   macro avg     0.7438    0.7330    0.7344       640\n",
            "weighted avg     0.7427    0.7406    0.7377       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 16, Train Loss: 0.202, Train Acc: 78.32%, Val. Loss: 0.277315, Val. Acc: 70.54%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8268    0.7312    0.7761       346\n",
            "           1     0.7216    0.8197    0.7675       294\n",
            "\n",
            "    accuracy                         0.7719       640\n",
            "   macro avg     0.7742    0.7755    0.7718       640\n",
            "weighted avg     0.7785    0.7719    0.7721       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 17, Train Loss: 0.218, Train Acc: 76.80%, Val. Loss: 0.237343, Val. Acc: 73.51%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7487    0.8179    0.7818       346\n",
            "           1     0.7595    0.6769    0.7158       294\n",
            "\n",
            "    accuracy                         0.7531       640\n",
            "   macro avg     0.7541    0.7474    0.7488       640\n",
            "weighted avg     0.7537    0.7531    0.7515       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 18, Train Loss: 0.242, Train Acc: 75.40%, Val. Loss: 0.265613, Val. Acc: 71.73%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6503    0.8815    0.7485       346\n",
            "           1     0.7602    0.4422    0.5591       294\n",
            "\n",
            "    accuracy                         0.6797       640\n",
            "   macro avg     0.7053    0.6618    0.6538       640\n",
            "weighted avg     0.7008    0.6797    0.6615       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 19, Train Loss: 0.235, Train Acc: 75.60%, Val. Loss: 0.368484, Val. Acc: 64.73%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7865    0.7775    0.7820       346\n",
            "           1     0.7416    0.7517    0.7466       294\n",
            "\n",
            "    accuracy                         0.7656       640\n",
            "   macro avg     0.7641    0.7646    0.7643       640\n",
            "weighted avg     0.7659    0.7656    0.7657       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 20, Train Loss: 0.211, Train Acc: 77.46%, Val. Loss: 0.250019, Val. Acc: 72.92%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6616    0.8757    0.7537       346\n",
            "           1     0.7637    0.4728    0.5840       294\n",
            "\n",
            "    accuracy                         0.6906       640\n",
            "   macro avg     0.7127    0.6743    0.6689       640\n",
            "weighted avg     0.7085    0.6906    0.6758       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 21, Train Loss: 0.243, Train Acc: 74.67%, Val. Loss: 0.352543, Val. Acc: 65.77%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.7007    0.8526    0.7692       346\n",
            "           1     0.7671    0.5714    0.6550       294\n",
            "\n",
            "    accuracy                         0.7234       640\n",
            "   macro avg     0.7339    0.7120    0.7121       640\n",
            "weighted avg     0.7312    0.7234    0.7167       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 22, Train Loss: 0.263, Train Acc: 73.01%, Val. Loss: 0.300368, Val. Acc: 68.90%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6956    0.8584    0.7684       346\n",
            "           1     0.7700    0.5578    0.6469       294\n",
            "\n",
            "    accuracy                         0.7203       640\n",
            "   macro avg     0.7328    0.7081    0.7077       640\n",
            "weighted avg     0.7297    0.7203    0.7126       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 23, Train Loss: 0.272, Train Acc: 72.41%, Val. Loss: 0.307731, Val. Acc: 68.60%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.6764    0.8699    0.7611       346\n",
            "           1     0.7692    0.5102    0.6135       294\n",
            "\n",
            "    accuracy                         0.7047       640\n",
            "   macro avg     0.7228    0.6901    0.6873       640\n",
            "weighted avg     0.7190    0.7047    0.6933       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 24, Train Loss: 0.237, Train Acc: 75.13%, Val. Loss: 0.337971, Val. Acc: 67.11%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8262    0.7283    0.7742       346\n",
            "           1     0.7194    0.8197    0.7663       294\n",
            "\n",
            "    accuracy                         0.7703       640\n",
            "   macro avg     0.7728    0.7740    0.7702       640\n",
            "weighted avg     0.7772    0.7703    0.7706       640\n",
            "\n",
            "640\n",
            "640\n",
            "Epoch: 25, Train Loss: 0.195, Train Acc: 78.72%, Val. Loss: 0.238981, Val. Acc: 73.36%\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.8187    0.7397    0.7772      2363\n",
            "           1     0.7447    0.8226    0.7817      2181\n",
            "\n",
            "    accuracy                         0.7795      4544\n",
            "   macro avg     0.7817    0.7811    0.7795      4544\n",
            "weighted avg     0.7832    0.7795    0.7794      4544\n",
            "\n",
            "4544\n",
            "4544\n",
            "Test Loss: 0.251, Test Acc: 77.40%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptEujeF-h-D-"
      },
      "source": [
        ""
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z--VB2UidT-"
      },
      "source": [
        "import pandas as pd\n",
        "test=pd.read_csv(\"processed_test.csv\")\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yrp9tRrgsVoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab31ad6c-6cd2-451c-b36a-c012ecba7840"
      },
      "source": [
        "final=[]\n",
        "for i in range(10):\n",
        "  # test_sen1 = \"This is one of the best creation of Nolan. I can say, it's his magnum opus. Loved the soundtrack and especially those creative dialogues.\"\n",
        "  # test_sen2 = \"Ohh, such a ridiculous movie. Not gonna recommend it to anyone. Complete waste of time and money.\"\n",
        "  test_sen1=test[\"boilerplate\"][i]\n",
        "  \n",
        "  test_sen1 = TEXT.preprocess(test_sen1)\n",
        "  test_sen1 = [[TEXT.vocab.stoi[x] for x in test_sen1]]\n",
        "\n",
        "  # test_sen2 = TEXT.preprocess(test_sen2)\n",
        "  # test_sen2 = [[TEXT.vocab.stoi[x] for x in test_sen2]]\n",
        "\n",
        "  test_sen = np.asarray(test_sen1)\n",
        "  test_sen = torch.LongTensor(test_sen)\n",
        "  test_tensor = Variable(test_sen, volatile=True)\n",
        "  test_tensor = test_tensor.cuda()\n",
        "  model.eval()\n",
        "  output = model(test_tensor, 1)\n",
        "  out = F.softmax(output, 1)\n",
        "\n",
        "  final.append(torch.argmax(out[0]).tolist())\n",
        "  # if (torch.argmax(out[0]) == 1):\n",
        "  #     print (\"Sentiment: Positive\")\n",
        "  # else:\n",
        "  #     print (\"Sentiment: Negative\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:15: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  from ipykernel import kernelapp as app\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9fzGmFhnssCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0d80e8a-87f0-4e6f-f088-0af940f47575"
      },
      "source": [
        "print(final)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 1, 1, 1, 0, 0, 0, 0, 1, 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhvRgEPftC3F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}